{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataset_processing\u001b[39;00m \u001b[39mimport\u001b[39;00m grasp, image\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgrasp_data\u001b[39;00m \u001b[39mimport\u001b[39;00m GraspDatasetBase\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mJacquardDataset\u001b[39;00m(GraspDatasetBase):\n",
      "File \u001b[0;32m~/Repositories/CoNSens/dataset_processing/grasp.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdraw\u001b[39;00m \u001b[39mimport\u001b[39;00m polygon\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mskimage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m peak_local_max\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gr_text_to_no\u001b[39m(l, offset\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from dataset_processing import grasp, image\n",
    "from .grasp_data import GraspDatasetBase\n",
    "\n",
    "\n",
    "class JacquardDataset(GraspDatasetBase):\n",
    "    \"\"\"\n",
    "    Dataset wrapper for the Jacquard dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, ds_rotate=0, **kwargs):\n",
    "        \"\"\"\n",
    "        :param file_path: Jacquard Dataset directory.\n",
    "        :param ds_rotate: If splitting the dataset, rotate the list of items by this fraction first\n",
    "        :param kwargs: kwargs for GraspDatasetBase\n",
    "        \"\"\"\n",
    "        super(JacquardDataset, self).__init__(**kwargs)\n",
    "\n",
    "        self.grasp_files = glob.glob(os.path.join(file_path, '*', '*_grasps.txt'))\n",
    "        self.grasp_files.sort()\n",
    "        self.length = len(self.grasp_files)\n",
    "\n",
    "        if self.length == 0:\n",
    "            raise FileNotFoundError('No dataset files found. Check path: {}'.format(file_path))\n",
    "\n",
    "        if ds_rotate:\n",
    "            self.grasp_files = self.grasp_files[int(self.length * ds_rotate):] + self.grasp_files[\n",
    "                                                                                 :int(self.length * ds_rotate)]\n",
    "\n",
    "        self.depth_files = [f.replace('grasps.txt', 'perfect_depth.tiff') for f in self.grasp_files]\n",
    "        self.rgb_files = [f.replace('perfect_depth.tiff', 'RGB.png') for f in self.depth_files]\n",
    "\n",
    "    def get_gtbb(self, idx, rot=0, zoom=1.0):\n",
    "        gtbbs = grasp.GraspRectangles.load_from_jacquard_file(self.grasp_files[idx], scale=self.output_size / 1024.0)\n",
    "        c = self.output_size // 2\n",
    "        gtbbs.rotate(rot, (c, c))\n",
    "        gtbbs.zoom(zoom, (c, c))\n",
    "        return gtbbs\n",
    "\n",
    "    def get_depth(self, idx, rot=0, zoom=1.0):\n",
    "        depth_img = image.DepthImage.from_tiff(self.depth_files[idx])\n",
    "        depth_img.rotate(rot)\n",
    "        depth_img.normalise()\n",
    "        depth_img.zoom(zoom)\n",
    "        depth_img.resize((self.output_size, self.output_size))\n",
    "        return depth_img.img\n",
    "\n",
    "    def get_rgb(self, idx, rot=0, zoom=1.0, normalise=True):\n",
    "        rgb_img = image.Image.from_file(self.rgb_files[idx])\n",
    "        rgb_img.rotate(rot)\n",
    "        rgb_img.zoom(zoom)\n",
    "        rgb_img.resize((self.output_size, self.output_size))\n",
    "        if normalise:\n",
    "            rgb_img.normalise()\n",
    "            rgb_img.img = rgb_img.img.transpose((2, 0, 1))\n",
    "        return rgb_img.img\n",
    "\n",
    "    def get_jname(self, idx):\n",
    "        return '_'.join(self.grasp_files[idx].split(os.sep)[-1].split('_')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Residual Network\n",
    "class ResNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=64)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual Layers\n",
    "        self.res1 = ResidualBlock(in_channels=64, out_channels=64)\n",
    "        self.res2 = ResidualBlock(in_channels=64, out_channels=64)\n",
    "        self.res3 = ResidualBlock(in_channels=64, out_channels=128, stride=2)\n",
    "        self.res4 = ResidualBlock(in_channels=128, out_channels=128)\n",
    "        self.res5 = ResidualBlock(in_channels=128, out_channels=256, stride=2)\n",
    "        self.res6 = ResidualBlock(in_channels=256, out_channels=256)\n",
    "        self.res7 = ResidualBlock(in_channels=256, out_channels=512, stride=2)\n",
    "        self.res8 = ResidualBlock(in_channels=512, out_channels=512)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = torch.nn.Linear(in_features=512, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional Layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Residual Layers\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        # Residual Layers\n",
    "        self.res = torch.nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.res = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride),\n",
    "                torch.nn.BatchNorm2d(num_features=out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional Layers\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "\n",
    "        # Residual Layers\n",
    "        x = self.res(x)\n",
    "\n",
    "        # Merge Layers\n",
    "        y = x + y\n",
    "        y = self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConSens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
